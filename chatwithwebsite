import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import pinecone
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Initialize Pinecone for Vector Database
pinecone.init(api_key="YOUR_PINECONE_API_KEY", environment="YOUR_PINECONE_ENV")
index_name = "website-rag"
if index_name not in pinecone.list_indexes():
    pinecone.create_index(index_name, dimension=768)
index = pinecone.Index(index_name)

# Initialize Embedding Model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Web Data Extraction Function
def extract_website_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')
    extracted_text = [para.get_text() for para in paragraphs if para.get_text()]
    return extracted_text

# Chunking Function (for better granularity)
def chunk_text(text, chunk_size=200):
    words = text.split()
    chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Ingest Website Data into Pinecone
def ingest_website_to_pinecone(url):
    data = extract_website_text(url)
    for idx, paragraph in enumerate(data):
        if paragraph:
            chunks = chunk_text(paragraph)
            for chunk in chunks:
                embedding = embedding_model.encode(chunk).tolist()
                index.upsert([(f"{url}para{idx}{chunks.index(chunk)}", embedding, {"text": chunk, "url": url})])

# Query Handling
def query_website(query):
    query_embedding = embedding_model.encode(query).tolist()
    search_results = index.query(query_embedding, top_k=5, include_metadata=True)
    retrieved_chunks = [result['metadata']['text'] for result in search_results['matches']]
    return retrieved_chunks

# Response Generation
def generate_response(query, retrieved_chunks):
    prompt_template = """
    You are an assistant that answers questions accurately using the provided context.
    Context:
    {context} 

    Question:
    {query} 

    Answer the question based on the context:
    """
    context = "\n".join(retrieved_chunks)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "query"])
    response = prompt.format(context=context, query=query)
    return response

# Comparison Queries
def handle_comparison_query(query, retrieved_chunks):
    # Process data for comparison (example: parsing for tabular comparisons)
    response = generate_response(query, retrieved_chunks)
    return response

# Run the RAG pipeline
def main():
    # Step 1: Ingest Website into Pinecone
    website_url = "https://example.com"  # Replace with the website URL
    ingest_website_to_pinecone(website_url)

    # Step 2: User Query
    user_query = "What is the mission of the organization?"  # Example query
    retrieved_chunks = query_website(user_query)

    # Step 3: Generate Response
    response = generate_response(user_query, retrieved_chunks)
    print("Response:\n", response)

    # Step 4: Handle Comparison Query (if required)
    comparison_query = "Compare the organization's mission and vision statements."  # Example comparison query
    comparison_response = handle_comparison_query(comparison_query, retrieved_chunks)
    print("\nComparison Response:\n", comparison_response)

# Execute the script
if __name__ == "__main__":
    main()
